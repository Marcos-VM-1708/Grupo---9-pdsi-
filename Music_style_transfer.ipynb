{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# bibliotecas"
      ],
      "metadata": {
        "id": "xtLBP7bfCx5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "import tensorflow as tf\n",
        "from keras.layers import *\n",
        "from scipy.io import wavfile\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from IPython.display import Audio, display\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "FMF7RKt6CxYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "kbHJ61gyCnUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJA1-9uCCicX"
      },
      "outputs": [],
      "source": [
        "def signal(caminho_do_sinal):\n",
        "    \"\"\"\n",
        "    Plota um sinal a partir de um arquivo especificado.\n",
        "\n",
        "    Parameters:\n",
        "    - caminho_do_sinal (str): O caminho do arquivo contendo o sinal. O arquivo deve ter duas colunas,\n",
        "      representando o tempo e os valores do sinal, respectivamente.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "\n",
        "    Raises:\n",
        "    - FileNotFoundError: Se o arquivo especificado não for encontrado.\n",
        "    - ValueError: Se houver um problema com o formato dos dados no arquivo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Carregar dados do sinal a partir do arquivo .wav\n",
        "        taxa_amostragem, dados_do_sinal = wavfile.read(caminho_do_sinal)\n",
        "\n",
        "        # Calcular o tempo correspondente a cada amostra\n",
        "        tempo = np.arange(0, len(dados_do_sinal)) / taxa_amostragem\n",
        "\n",
        "        # Plotar o sinal\n",
        "        plt.plot(tempo, dados_do_sinal)\n",
        "        plt.title('Sinal de Áudio')\n",
        "        plt.xlabel('Tempo (s)')\n",
        "        plt.ylabel('Amplitude')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Arquivo não encontrado: {caminho_do_sinal}\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Erro ao processar dados do sinal: {e}\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def audio_to_imagem(arquivo_path, imagem_size):\n",
        "    \"\"\"\n",
        "    Converte um arquivo de áudio em uma representação visual do espectro de frequência em uma imagem.\n",
        "\n",
        "    Parâmetros:\n",
        "    - caminho_arquivo (str): O caminho do arquivo de áudio a ser processado.\n",
        "    - tamanho_imagem (tuple): Um par de valores [altura, largura] para especificar o tamanho da imagem resultante.\n",
        "\n",
        "    Retorno:\n",
        "    - imagem_espectro (PIL.Image.Image): Imagem em escala de cinza representando o espectro de frequência.\n",
        "    - valor_min_mag (float): Valor mínimo da magnitude do espectro original.\n",
        "    - valor_max_mag (float): Valor máximo da magnitude do espectro original.\n",
        "    - fase_transformada (numpy.ndarray): Fase da transformada de Fourier de curto prazo.\n",
        "    \"\"\"\n",
        "\n",
        "    sinal_audio, taxa_amostragem = librosa.load(arquivo_path)\n",
        "\n",
        "    # TRANSFORMADA DE FOURIER - (STFT)...\n",
        "    espectrograma = librosa.stft(sinal_audio)\n",
        "\n",
        "    magnitude, fase_transformada = librosa.magphase(espectrograma)\n",
        "    magnitude_log = np.log1p(magnitude)\n",
        "\n",
        "    # NORMALIZAÇÃO DA MAGNITUDE...\n",
        "    valor_min_mag, valor_max_mag = magnitude_log.min(), magnitude_log.max()\n",
        "    magnitude_normalizada = (magnitude_log - valor_min_mag) / (valor_max_mag - valor_min_mag)\n",
        "\n",
        "    # RESHAPE DA MAGNITUDE...\n",
        "    magnitude_normalizada = magnitude_normalizada[:imagem_size[0], :imagem_size[1]]\n",
        "\n",
        "    #  CONVERTE PARA IMAGEM...\n",
        "    dados_imagem = (magnitude_normalizada * 255).astype(np.uint8)\n",
        "    imagem_espectro = Image.fromarray(dados_imagem, mode='L')\n",
        "\n",
        "    return imagem_espectro, valor_min_mag, valor_max_mag, fase_transformada\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def image_to_audio(img, mag_min, mag_max):\n",
        "    \"\"\"\n",
        "    Converte uma imagem em escala de cinza para um sinal de áudio.\n",
        "\n",
        "    Parâmetros:\n",
        "    - img (PIL.Image.Image): A imagem em escala de cinza representando o espectro de frequência.\n",
        "    - mag_min (float): Valor mínimo da magnitude do espectro original.\n",
        "    - mag_max (float): Valor máximo da magnitude do espectro original.\n",
        "\n",
        "    Retorno:\n",
        "    - sinal_audio (numpy.ndarray): O sinal de áudio reconstruído a partir da imagem.\n",
        "    \"\"\"\n",
        "\n",
        "    mag_norm = np.array(img, dtype=np.float32) / 255\n",
        "    mag = mag_norm * (mag_max - mag_min) + mag_min\n",
        "    mag = np.exp(mag) - 1\n",
        "\n",
        "    # RECONSTRUIR O SINAL DE ÁUDIO USANDO O ALGORITMO GRIFFIN-LIM...\n",
        "    sinal_audio = librosa.griffinlim(mag)\n",
        "    return sinal_audio\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def kernel_initializer(shape, dtype=None):\n",
        "    \"\"\"\n",
        "    Inicializa um kernel de maneira personalizada para uma camada Conv2D.\n",
        "\n",
        "    Parameters:\n",
        "        shape (tuple): Formato do kernel.\n",
        "        dtype (tf.dtypes.DType, opcional): Tipo de dados (padrão é None).\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Tensor constante representando o kernel inicializado.\n",
        "    \"\"\"\n",
        "    # CALCULA DESVIO PADRÃO IDEAL...\n",
        "    std = np.sqrt(2) * np.sqrt(2.0 / ((1025 + 4096) * 11)) # CHANNELS + FILTERS\n",
        "    # GERA UMA MATRIZ DE DIMENSÕES (1, 11, CHANNELS, FILTERS)...\n",
        "    kernel = np.random.randn(1, 11, shape[-2], shape[-1]) * std\n",
        "    return tf.constant(kernel, dtype=dtype)\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def create_model(input_shape, FILTERS):\n",
        "    \"\"\"\n",
        "       Cria um modelo de CNN com uma camada Conv2D.\n",
        "\n",
        "       Parameters:\n",
        "           input_shape (tuple): Formato da entrada.\n",
        "           FILTERS (int): Número de filtros na camada convolucional.\n",
        "\n",
        "       Returns:\n",
        "           tf.keras.models.Model: Modelo de CNN construído.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    outputs = Conv2D(\n",
        "        filters=FILTERS,\n",
        "        kernel_size=(1, 11),\n",
        "        padding='same',\n",
        "        activation='relu',\n",
        "        kernel_initializer = kernel_initializer\n",
        "    )(inputs)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------def custom_kernel_initializer(shape, dtype=None, FILTERS, CHANNELS):\n",
        "\n",
        "def gram_matrix(x):\n",
        "    \"\"\"\n",
        "    Calcula a matriz de Gram para o tensor.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Tensor de entrada.\n",
        "\n",
        "    Returns:\n",
        "    Matriz de Gram.\n",
        "    \"\"\"\n",
        "    feats = tf.reshape(x, (-1, tf.shape(x)[-1]))\n",
        "    return tf.matmul(tf.transpose(feats), feats)\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------def custom_kernel_initializer(shape, dtype=None, FILTERS, CHANNELS):\n",
        "\n",
        "def get_style_loss(style_image_features, generated_image_features):\n",
        "    \"\"\"\n",
        "    Calcula a perda de estilo entre duas representações de imagem.\n",
        "\n",
        "    Parameters:\n",
        "    - style_image_features: Representação de estilo da imagem de referência.\n",
        "    - generated_image_features: Representação de estilo da imagem gerada.\n",
        "\n",
        "    Returns:\n",
        "    Perda de estilo.\n",
        "    \"\"\"\n",
        "    gram_style = gram_matrix(style_image_features)\n",
        "    gram_generated = gram_matrix(generated_image_features)\n",
        "\n",
        "    # Adicionando um pequeno termo epsilon para evitar divisão por zero\n",
        "    epsilon = 1e-8\n",
        "    return tf.sqrt(tf.reduce_sum(tf.square(gram_style - gram_generated)) + epsilon)\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------def custom_kernel_initializer(shape, dtype=None, FILTERS, CHANNELS):\n",
        "\n",
        "def get_content_loss(content_image_features, generated_image_features):\n",
        "    \"\"\"\n",
        "    Calcula a perda de conteúdo entre duas representações de imagem.\n",
        "\n",
        "    Parameters:\n",
        "    - content_image_features: Representação de conteúdo da imagem de referência.\n",
        "    - generated_image_features: Representação de conteúdo da imagem gerada.\n",
        "\n",
        "    Returns:\n",
        "    Perda de conteúdo.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-8\n",
        "    return tf.sqrt(tf.reduce_sum(tf.square(content_image_features - generated_image_features)) + epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ㅤ"
      ],
      "metadata": {
        "id": "ZR4JhwavDILe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SIZE = (1025, 430)\n",
        "\n",
        "# AQUI ABRIMOS DUAS MUSICAS, UMA QUE TERA SUA ESTRUTURA MANTIDA\n",
        "# E OUTRA QUE SERVIRA DE ESTUDO PARA UMA APROXIMAÇÃO DA TRASFERENCIA DE ESTILO.\n",
        "SOUND = \"/content/Bach.wav\"\n",
        "STYLE = \"/content/Bbno Edamame.wav\"\n",
        "\n",
        "# VISUALIZANDO O SINAL...\n",
        "signal(SOUND)\n",
        "signal(STYLE)\n"
      ],
      "metadata": {
        "id": "xh1a8LnJDHhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_img, mag_min, mag_max, phase = audio_to_imagem(SOUND, SIZE)\n",
        "style_img, _, _, _ = audio_to_imagem(STYLE, SIZE)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('signal music')\n",
        "plt.imshow(content_img)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('sgnal style')\n",
        "plt.imshow(style_img)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fUU0-P5QD2P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE MODEL...\n",
        "\n",
        "content_np = np.array(content_img).T[None, None, :, :]\n",
        "style_np = np.array(style_img).T[None, None, :, :]\n",
        "\n",
        "content_tensor = tf.convert_to_tensor(content_np, dtype=tf.float32)\n",
        "style_tensor = tf.convert_to_tensor(style_np, dtype=tf.float32)\n",
        "\n",
        "BATCH, HEIGHT, WIDTH, CHANNELS = content_tensor.shape\n",
        "FILTERS = 4096\n",
        "\n",
        "input_shape = (HEIGHT, WIDTH, CHANNELS)\n",
        "\n",
        "model = create_model(input_shape, FILTERS)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "V0SDc6YAD7Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAING...\n",
        "content_features = model(content_tensor)\n",
        "style_features = model(style_tensor)\n",
        "\n",
        "gen_np = tf.random.normal((1, *input_shape))\n",
        "gen = tf.Variable(gen_np)\n",
        "\n",
        "steps_counter = 0\n",
        "\n",
        "STEPS = 5000\n",
        "\n",
        "optimizer = Adam(learning_rate=1)\n",
        "\n",
        "\n",
        "for i in range(STEPS):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(gen)\n",
        "\n",
        "        gen_features = model(gen)\n",
        "\n",
        "        content_loss = get_content_loss(gen_features, content_features)\n",
        "        style_loss = get_style_loss(gen_features, style_features) * 0.001\n",
        "\n",
        "        loss = content_loss + style_loss\n",
        "\n",
        "    gradients = tape.gradient(loss, [gen])\n",
        "    optimizer.apply_gradients(zip(gradients, [gen]))\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"Step: {i} | loss: {loss.numpy()} | {content_loss.numpy()} | {style_loss.numpy()}\")\n",
        "\n",
        "steps_counter += STEPS"
      ],
      "metadata": {
        "id": "4dA1dpklEd0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_np = np.squeeze(gen.numpy()).T\n",
        "gen_img = Image.fromarray(gen_np).convert('L')\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Content\")\n",
        "plt.imshow(content_img)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Style\")\n",
        "plt.imshow(style_img)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Generated\")\n",
        "plt.imshow(gen_img)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "x = image_to_audio(gen_img, mag_min, mag_max)\n",
        "\n",
        "gen_img.convert('RGB').save('ouput.jpg')\n",
        "np.save(f'weights.npy', gen.numpy())\n",
        "sf.write(f'output.mp3', x, 22050)"
      ],
      "metadata": {
        "id": "bPq_v7RqEpY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}